# 1. 机器学习笔记

- [1. 机器学习笔记](#1-机器学习笔记)
- [2. 基础问题](#2-基础问题)
  - [2.1. 过拟合与欠拟合](#21-过拟合与欠拟合)
  - [2.2. 正则化](#22-正则化)
  - [2.3. AUC](#23-auc)
  - [2.4. 超参搜索](#24-超参搜索)
  - [2.5. PCA和LDA](#25-pca和lda)
- [3. 树模型](#3-树模型)
  - [3.1. xgbost和gbdt的区别](#31-xgbost和gbdt的区别)
  - [3.2. xgboost和lgb的区别](#32-xgboost和lgb的区别)
- [4. DNN](#4-dnn)
  - [4.1. 参数是否可以初始化为0](#41-参数是否可以初始化为0)
  - [4.2. relu](#42-relu)
  - [4.3. 梯度消失和梯度膨胀](#43-梯度消失和梯度膨胀)
- [5. CNN && CV](#5-cnn--cv)
  - [5.1. CNN适合处理什么问题](#51-cnn适合处理什么问题)
  - [5.2. CNN为什么具有平移不变性](#52-cnn为什么具有平移不变性)
  - [5.3. Pooling操作是什么？有几种？作用是什么？](#53-pooling操作是什么有几种作用是什么)
  - [5.4. BatchNormalization](#54-batchnormalization)
  - [5.5. 卷积](#55-卷积)
  - [5.6. 怎么看CNN的每个神经元都学到了什么。](#56-怎么看cnn的每个神经元都学到了什么)
  - [5.7. Resnet skip-connection](#57-resnet-skip-connection)
- [6. RNN&&NLP](#6-rnnnlp)
  - [6.1. RNN原理？RNN适合解决什么类型问题？为什么？](#61-rnn原理rnn适合解决什么类型问题为什么)
  - [6.2. LSTM如何实现长短期记忆功能？](#62-lstm如何实现长短期记忆功能)
  - [6.3. GRU跟LSTM有什么异同](#63-gru跟lstm有什么异同)
  - [6.4. RNN的长期以来问题是什么？怎么解决](#64-rnn的长期以来问题是什么怎么解决)
  - [6.5. Seq2Seq模型](#65-seq2seq模型)
- [7. 强化学习](#7-强化学习)
- [8. 参考资料](#8-参考资料)

# 2. 基础问题

## 2.1. 过拟合与欠拟合
过拟合：训练集效果好，测试机效果差。

- 欠拟合
  - 增加特征
  - 增加模型复杂度
  - 减少正则项稀疏
- 过拟合
  - 提高样本量
  - 简化模型
  - 加入正则项或提高惩罚稀疏
  - 是用集成学习
  - dropout
  - early stopping
  - [label smoothing(标签平滑)](https://blog.csdn.net/qq_40176087/article/details/121519888) 避免模型过于自信,让标签不绝对

## 2.2. 正则化
- 数据增强
- L2正则
- L1正则
- dropout，神经元被丢弃的概率为 1 − p，减少神经元之间的共适应。一种廉价的 Bagging 集成近似方法。减少训练节点提高了学习速度。
- drop connect， 在 Drop Connect 的过程中需要将网络架构权重的一个随机选择子集设置为零，取代了在 Dropout 中对每个层随机选择激活函数的子集设置为零的做法。
- 随机pooling
- early stopping

## 2.3. AUC
- ROC曲线下的面积。ROC是用FPR和TPR作为x，y绘制的曲线。
- AUC取值一般在0.5到1之间
- AUC更关注序，对于样本不均衡情况，也能给出合理的评价
- 有几种计算方式
  - 计算ROC曲线下的面积。只能用近似方法去算。
  - 统计逆序对个数。按照pred升序排列。得到正样本的rank累计值R。 (R-M(M-1)/2)/(M*N)。这里M是正样本个数，N是负样本个数。分母是正负样本对的个数，分子是逆序对的个数。

## 2.4. 超参搜索

[超参数搜索的方式](https://zhuanlan.zhihu.com/p/304373868)
- 人工调参(babysitting)
- 网格搜索(Grid Search)，先用较大步长在较大范围搜索，确定可能的位置，然后逐渐缩小搜索范围和步长。简单有效，但是耗时久，目标函数非凸时容易miss全局最优。
- 随机搜索(Random Search)，在搜索范围内随机选取样本点，样本集足够大也能找到全局最优或者近似解。优点是快，但是也可能miss全局最优。
- 贝叶斯优化，对目标函数的形状进行学习，找到使目标函数向全局最优值提升的参数。优点是充分利用之前的信息。缺点是容易陷入局部最优。
  - [SMBO](https://zhuanlan.zhihu.com/p/53826787)(Sequential model-based optimization)
- 进化算法
  - 基础理论：[帕累托最优](https://zhuanlan.zhihu.com/p/54691447) 
  - [CEM](https://blog.csdn.net/ppp8300885/article/details/80567682)(Cross Entropy Method)
  - [PSO](https://cloud.tencent.com/developer/article/1424756)(Particle Swarm Optimization, 粒子群算法)
  - [NES](https://mofanpy.com/tutorials/machine-learning/evolutionary-algorithm/evolution-strategy-natural-evolution-strategy/)(Natural Evolution Strategy)

## 2.5. PCA和LDA

PCA无监督，基于方差降维，去除冗余维度。LDA有监督，类内方差最小。


# 3. 树模型
## 3.1. xgbost和gbdt的区别
- GBDT是机器学习算法，xgboost是该算法的工程实现
- 传统GBDT是用CART作为基分类器，XGBOOST还支持线性分类器，比如LR或者线性回归
- GBDT只用了一阶导数。xgboost用了二阶泰勒展开。支持自定义损失函数，但是要求一阶、二阶可导
- GBDT每次迭代用全量数据，xgboost有行采样和列采样
- xgboost的目标函数里面有正则项，相当于预剪枝
- xgboost支持对缺失值处理
- xgboost支持并行。并行是在特征粒度上
- 支持统计直方图做近似计算

## 3.2. xgboost和lgb的区别

不同：
- xgboost是level-wise，lightgbm使用leaf-wise，这个可以提升训练速度。但是其实xgboost已经支持leaf-wise
  - leaf-wise的问题是可能忽略未来有潜力的节点
- xgboost单机默认是exact greedy，搜索所有的可能分割点。分布式是dynamic histogram，每一轮迭代重新estimate 潜在split candidate。LightGBM和最近的FastBDT都采取了提前histogram binning再在bin好的数据上面进行搜索。在限定好candidate splits。lightgbm在pre-bin之后的histogram的求和用了一个非常巧妙的减法trick，省了一半的时间。
  - 提前限定分割点然后快速求histogram的方法，实际影响不确定。理论上树越深，需要的潜在分割点越多，可能需要动态训练来更新潜在分割点
- xgboost主要是特征并行，lightgbm是有数据并行、特征并行、投票并行。当时其实xgboost也支持数据并行了。
- lightgbm支持分类特征的many vs many，用G/H排序，然后再分桶
参考：
- [如何看待微软新开源的LightGBM?-陈天奇和柯国霖都有回答](https://www.zhihu.com/question/51644470)


# 4. DNN
## 4.1. 参数是否可以初始化为0
不可以。对于全连接多层神经网络，初始化为0，反向传播过程如下：
- 1）第1次反向传播，仅最后一层权重值更新，前面的权重值不更新（仍为0）
- 2）第2次反向传播，最后一层和倒数第二层权重值更新，前面的权重值不更新（仍为0）
- 3）以此类推，若干次反向传播之后，达到如下状态：
- a）输入层和隐层之间，链接在同一个输入节点的权重值相同，链接在不同输入节点的权重值不同；
- b）两个隐层之间，所有权重值相同
- 输出层和隐层之间，链接在同一个输出节点的权重值相同，链接在不同输出节点的权重值不同
以上分析针对sigmoid激活函数，tanh函数下权重值无论怎么训练都保持为0.relu估计也是都为0
  
参考：[关于神经网络参数初始化为全0的思考](https://zhuanlan.zhihu.com/p/32063750)

## 4.2. relu
- y=max(0,x)
- relu在0处是不可导的，TensorFlow实现默认0点的导数为0
- relu是非饱和激活函数。sigmoid和tanh是饱和激活函数。
  - relu的优势：
    - 非饱和激活函数可以解决梯度消失问题，提供相对宽的激活边界
    - 能加快收敛速度
    - 单次抑制提供了稀疏表达能力，防止过拟合
  - 缺点
    - 训练很脆弱，很容易权重就为0了，也就是神经元死亡
- relu变种
  - Leaky Relu 给所有负值一个非零斜率
  - [PRelu](https://blog.csdn.net/shuzfan/article/details/51345832) 是Leaky Relu的一个变体，负值部分的斜率是根据数据来定的。斜率a是用带动量的更新方式。论文中初始值0.25，动量= 动量系数*动量+学习率*偏导
  - RRelu 也是Leaky Relu的一个变体，负值斜率在训练中是随机的，在测试中变成固定值。权重a是一个均匀分布U(l,u) l,u∈[0,1) 

参考：[激活函数ReLU、Leaky ReLU、PReLU和RReLU](https://blog.csdn.net/qq_23304241/article/details/80300149)

## 4.3. 梯度消失和梯度膨胀
原因：反向传播是根据链式法则，连乘每一层的偏导。每一层的偏导是激活函数的导数乘以当前节点的权重。如果每一层的偏导都同时偏向一个方向，都大于1会导致最终梯度爆炸，都小于1会导致梯度消失(变为0)

解决办法：relu可以缓解。但是用了relu的网络也还是存在梯度消失的问题，Leaky ReLU就此应运而生。
参考：
- [梯度消失与梯度爆炸的原因](https://zhuanlan.zhihu.com/p/25631496)
- [在使用relu的网络中，是否还存在梯度消失的问题？]（https://www.zhihu.com/question/49230360）

# 5. [CNN](https://zhuanlan.zhihu.com/p/29605049) && CV
## 5.1. CNN适合处理什么问题
CNN通过卷积层+pooling层不断堆积，从小的pattern开始不断识别到大的pattern，从而识别整张图像。
CNN利用了图像的三个性质：
- 图像的pattern通常比整张图像小
- 通用的patterns会出现在图像的不同区域
- 对图像进行子采样并不影响图像的识别

## 5.2. CNN为什么具有平移不变性
参数共享的物理意义是使得卷积层具有平移等变性。在卷积神经网路中，卷积核中的每一个元素将作用于每一次局部输入的特定位置上。

假如图像中有一只猫，无论它出现在图像中的任何位置，我们都应该将它识别为猫，也就是说神经网络的输出对于平移变换来说应当是等变的。

## 5.3. Pooling操作是什么？有几种？作用是什么？
- 将Pooling核覆盖区域中所有制的平均值(最大值)作为输出。
- 有average-pooling、max-pooling、stochastic-pooling(对输入数据中的元素按照一定概率大小随机选择，元素值大的activattion被选中的概率大)。Pooling操作后的结果相比起输入减小了，是一种降采样操作。
- 作用：特征不变形。数据降维。

为什么CNN需要Pooling？因为图片的信号过于丰富，需要降维来减少计算量。图像信息存在冗余，子采样不影响图像识别。

## 5.4. BatchNormalization
对每个Batch的mean、var做指数加权平均用于作为整个样本空间的mean、var的近似估计。做了normalization之后所有值的丰富都在0,1，同层的神经元的差异性就没了，所以需要在修正回来，修正的参数是可学习的。

## 5.5. 卷积 
卷积的特性
- 稀疏交互。每个神经元的只跟上一层的某些神经元连接（vs DNN全连接），用到较少参数
- 参数共享。同一层的不同神经元之间共享部分权重，用到比原来更少的参数

## 5.6. 怎么看CNN的每个神经元都学到了什么。
- [特征可视化](https://zhuanlan.zhihu.com/p/443264651)指的就是将那些可以引起CNN Kernel最大反应的图像制作出来.

## 5.7. Resnet skip-connection  
添加一个捷径，来自深层的梯度能直接畅通无阻地通过，去到上一层，使得浅层的网络层参数等到有效的训练！

# 6. RNN&&NLP
## 6.1. RNN原理？RNN适合解决什么类型问题？为什么？
RNN能够很好地处理变长并且有序的输入序列(例如，文本数据)。它模拟了人阅读一篇文章的顺序，从前到后阅读文章中的每一个单词，将前面阅读到的有用信息编码到状态变量中去，从而拥有一定的记忆能力，可以更好地理解之后的文本。下一时刻的状态于当前输入以及当前状态有关。

## 6.2. LSTM如何实现长短期记忆功能？
LSTM可以对有价值的信息进行长期记忆。与RNN不同的是，LSTM记忆单元c的转移不一定完全取决于激活函数计算得到的状态，还由输入门和遗忘门共同控制。在一个训练好的LSTM模型中，当输入序列中没有重要信息时，遗忘门的值接近于1，输入门的值接近于0，表示过去的记忆被完整保存，而输入信息被放弃，从而实现长期记忆功能。当输入序列中存在重要信息时，LSTM应把他存入记忆中，此时输入门接近于1；当输入序列中存在重要信息且该信息意味着之前的记忆不再重要时，输入门的值接近1，遗忘门的值接近0。

参考：[理解 LSTM 网络](https://www.jianshu.com/p/9dc9f41f0b29)

LSTM参数数量？若输入的长度=m,隐藏层的长度=n，则一个LSTM层的参数个数是(n*(n+m)+n)*4。输入、记忆门、遗忘门、输出。(把状态遍历和输出给到下一层)
参考：[关于LSTM的神经元数及参数个数](https://blog.csdn.net/code_wxy/article/details/100887492)

## 6.3. GRU跟LSTM有什么异同
相同点：
  - 都有门
  - 遗忘门或者更新门选择不更新memeory，网络会一直记住之前的重要特征。
- 不同
  - GRU比LSTM比少了一个门。将遗忘门和输入门合成了一个单一的更新门
  - GRU不区分长期记忆和短期记忆了
  - LSTM有一个输出门控制memory的曝光程度，GRU是直接输出
  - GRU通过重置门来控制从H(t-1)中得到的信息粒度，LSTM是直接输入。
  - 相同参数量，GRU比LSTM表现稍好
参考：[门控循环单元（GRU）](https://zh.d2l.ai/chapter_recurrent-modern/gru.html#)

## 6.4. RNN的长期以来问题是什么？怎么解决
长期依赖问题是：随着输入序列的增长，模型的性能发生显著下降，RNN难以捕捉长距离输入之间的依赖。从结构上来看，理论上RNN可以学习捕捉到长距离依赖，但是实践中使用BPTT算法学习的RNN并不能成功捕捉长距离的医疗关系，主要源于深度神经网络中的梯度消失。

解决方法：
梯度爆炸：权重矩阵连乘，特征值幅度大于1，于是指数级增长，加tanh可以缓解
  - 梯度截断 
梯度消失：权重矩阵连乘，特征幅度小于0，于是衰减到0
  - LSTM、GRU等模型加入门控机制，捕捉长期记忆，很大程度上弥补了梯度消失
  - 残差结构
  - 设计多个时间尺度的模型：在细粒度的时间尺度上处理近期信息、在粗粒度时间尺度上处理远期的信息。得到粗粒度时间尺度方法1跳跃链接：增加从远期的隐变量到当前隐变量的直接连接；2. 是删除连接：主动删除时间跨度为 1 的连接，并用更长的连接替换。

## 6.5. Seq2Seq模型 
seq2seq模型是将一个序列信号，通过编码和解码生成一个新的序列信号，输入和输出序列的长度实现并不知道。seq2seq的模型的核心思想由编码输入和解码输出两个环节构成。在经典的实现中，编码器和解码器各由一个循环神经网络构成，两个循环神经网络是共同训练的。

# 7. 强化学习


# 8. 参考资料

- [算法工程师-机器学习面试题总结](https://github.com/zhengjingwei/machine-learning-interview) 总结得很全
- [人工智能学习资料书籍](https://github.com/leerumor/ai-study)